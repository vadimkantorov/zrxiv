{
  "title": "Deep Neural Networks as Gaussian Processes",
  "author": [
    "Jaehoon Lee",
    "Yasaman Bahri",
    "Roman Novak",
    "Samuel S. Schoenholz",
    "Jeffrey Pennington",
    "Jascha Sohl-Dickstein"
  ],
  "abstract": "  It has long been known that a single-layer fully-connected neural network\nwith an i.i.d. prior over its parameters is equivalent to a Gaussian process\n(GP), in the limit of infinite network width. This correspondence enables exact\nBayesian inference for infinite width neural networks on regression tasks by\nmeans of evaluating the corresponding GP. Recently, kernel functions which\nmimic multi-layer random neural networks have been developed, but only outside\nof a Bayesian framework. As such, previous work has not identified that these\nkernels can be used as covariance functions for GPs and allow fully Bayesian\nprediction with a deep neural network.\n  In this work, we derive the exact equivalence between infinitely wide deep\nnetworks and GPs. We further develop a computationally efficient pipeline to\ncompute the covariance function for these GPs. We then use the resulting GPs to\nperform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10.\nWe observe that trained neural network accuracy approaches that of the\ncorresponding GP with increasing layer width, and that the GP uncertainty is\nstrongly correlated with trained network prediction error. We further find that\ntest performance increases as finite-width trained networks are made wider and\nmore similar to a GP, and thus that GP predictions typically outperform those\nof finite-width networks. Finally we connect the performance of these GPs to\nthe recent theory of signal propagation in random neural networks.\n",
  "id": "1711.00165",
  "date": 1541590579,
  "url": "https://arxiv.org/abs/1711.00165",
  "tags": [
    "giant"
  ]
}