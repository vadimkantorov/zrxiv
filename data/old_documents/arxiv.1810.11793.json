{
  "title": "Robust Audio Adversarial Example for a Physical Attack",
  "authors": [
    "Hiromu Yakura",
    "Jun Sakuma"
  ],
  "abstract": "We propose a method to generate audio adversarial examples that can attack a\nstate-of-the-art speech recognition model in the physical world. Previous work\nassumes that generated adversarial examples are directly fed to the recognition\nmodel, and is not able to perform such a physical attack because of\nreverberation and noise from playback environments. In contrast, our method\nobtains robust adversarial examples by simulating transformations caused by\nplayback or recording in the physical world and incorporating the\ntransformations into the generation process. Evaluation and a listening\nexperiment demonstrated that our adversarial examples are able to attack\nwithout being noticed by humans. This result suggests that audio adversarial\nexamples generated by the proposed method may become a real threat.",
  "id": "arxiv.1810.11793",
  "url": "https://arxiv.org/abs/1810.11793",
  "pdf": "https://arxiv.org/pdf/1810.11793",
  "bibtex": "@misc{yakura2018_arxiv:1810.11793,\n    title = {Robust Audio Adversarial Example for a Physical Attack},\n    author = {Hiromu Yakura and Jun Sakuma},\n    year = {2018},\n    archiveprefix = {arXiv},\n    eprint = {1810.11793},\n    pdf = {https://arxiv.org/pdf/1810.11793},\n    url = {https://arxiv.org/abs/1810.11793}\n}",
  "source": "arxiv.org",
  "date": 1562139072,
  "tags": [],
  "api": "https://export.arxiv.org/api/query?id_list=1810.11793"
}