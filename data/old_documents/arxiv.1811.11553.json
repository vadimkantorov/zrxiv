{
  "title": "Strike (with) a Pose: Neural Networks Are Easily Fooled by Strange Poses\n  of Familiar Objects",
  "author": [
    "Michael A. Alcorn",
    "Qi Li",
    "Zhitao Gong",
    "Chengfei Wang",
    "Long Mai",
    "Wei-Shinn Ku",
    "Anh Nguyen"
  ],
  "abstract": "  Despite excellent performance on stationary test sets, deep neural networks\n(DNNs) can fail to generalize to out-of-distribution (OoD) inputs, including\nnatural, non-adversarial ones, which are common in real-world settings. In this\npaper, we present a framework for discovering DNN failures that harnesses 3D\nrenderers and 3D models. That is, we estimate the parameters of a 3D renderer\nthat cause a target DNN to misbehave in response to the rendered image. Using\nour framework and a self-assembled dataset of 3D objects, we investigate the\nvulnerability of DNNs to OoD poses of well-known objects in ImageNet. For\nobjects that are readily recognized by DNNs in their canonical poses, DNNs\nincorrectly classify 97% of their pose space. In addition, DNNs are highly\nsensitive to slight pose perturbations. Importantly, adversarial poses transfer\nacross models and datasets. We find that 99.9% and 99.4% of the poses\nmisclassified by Inception-v3 also transfer to the AlexNet and ResNet-50 image\nclassifiers trained on the same ImageNet dataset, respectively, and 75.5%\ntransfer to the YOLOv3 object detector trained on MS COCO.\n",
  "id": "arxiv.1811.11553",
  "url": "https://arxiv.org/abs/1811.11553",
  "pdf": "https://arxiv.org/pdf/1811.11553",
  "source": "arxiv.org",
  "date": 1543509013,
  "tags": []
}