{
  "title": "Self-Attention with Relative Position Representations",
  "authors": [
    "Peter Shaw",
    "Jakob Uszkoreit",
    "Ashish Vaswani"
  ],
  "abstract": "  Relying entirely on an attention mechanism, the Transformer introduced by\nVaswani et al. (2017) achieves state-of-the-art results for machine\ntranslation. In contrast to recurrent and convolutional neural networks, it\ndoes not explicitly model relative or absolute position information in its\nstructure. Instead, it requires adding representations of absolute positions to\nits inputs. In this work we present an alternative approach, extending the\nself-attention mechanism to efficiently consider representations of the\nrelative positions, or distances between sequence elements. On the WMT 2014\nEnglish-to-German and English-to-French translation tasks, this approach yields\nimprovements of 1.3 BLEU and 0.3 BLEU over absolute position representations,\nrespectively. Notably, we observe that combining relative and absolute position\nrepresentations yields no further improvement in translation quality. We\ndescribe an efficient implementation of our method and cast it as an instance\nof relation-aware self-attention mechanisms that can generalize to arbitrary\ngraph-labeled inputs.\n",
  "id": "arxiv.1803.02155",
  "url": "https://arxiv.org/abs/1803.02155",
  "pdf": "https://arxiv.org/pdf/1803.02155",
  "source": "arxiv.org",
  "date": 1544825585,
  "tags": []
}