{
  "title": "A Differentiable Gaussian-like Distribution on Hyperbolic Space for Gradient-Based Learning",
  "authors": [
    "Yoshihiro Nagano",
    "Shoichiro Yamaguchi",
    "Yasuhiro Fujita",
    "Masanori Koyama"
  ],
  "abstract": "  Hyperbolic space is a geometry that is known to be well-suited for\nrepresentation learning of data with an underlying hierarchical structure. In\nthis paper, we present a novel hyperbolic distribution called\n\\textit{pseudo-hyperbolic Gaussian}, a Gaussian-like distribution on hyperbolic\nspace whose density can be evaluated analytically and differentiated with\nrespect to the parameters. Our distribution enables the gradient-based learning\nof the probabilistic models on hyperbolic space that could never have been\nconsidered before. Also, we can sample from this hyperbolic probability\ndistribution without resorting to auxiliary means like rejection sampling. As\napplications of our distribution, we develop a hyperbolic-analog of variational\nautoencoder and a method of probabilistic word embedding on hyperbolic space.\nWe demonstrate the efficacy of our distribution on various datasets including\nMNIST, Atari 2600 Breakout, and WordNet.\n",
  "id": "arxiv.1902.02992",
  "url": "https://arxiv.org/abs/1902.02992",
  "pdf": "https://arxiv.org/pdf/1902.02992",
  "bibtex": "@misc{nagano2019_arxiv:1902.02992,\n    title = {A Differentiable Gaussian-like Distribution on Hyperbolic Space for Gradient-Based Learning},\n    author = {Yoshihiro Nagano, Shoichiro Yamaguchi, Yasuhiro Fujita, Masanori Koyama},\n    year = {2019},\n    archiveprefix = {arXiv},\n    eprint = {1902.02992},\n    pdf = {https://arxiv.org/pdf/1902.02992},\n    url = {https://arxiv.org/abs/1902.02992}\n}",
  "source": "arxiv.org",
  "date": 1550058445,
  "tags": []
}