{
  "title": "Learning to Compose Dynamic Tree Structures for Visual Contexts",
  "authors": [
    "Kaihua Tang",
    "Hanwang Zhang",
    "Baoyuan Wu",
    "Wenhan Luo",
    "Wei Liu"
  ],
  "abstract": "  We propose to compose dynamic tree structures that place the objects in an\nimage into a visual context, helping visual reasoning tasks such as scene graph\ngeneration and visual Q&A. Our visual context tree model, dubbed VCTree, has\ntwo key advantages over existing structured object representations including\nchains and fully-connected graphs: 1) The efficient and expressive binary tree\nencodes the inherent parallel/hierarchical relationships among objects, e.g.,\n\"clothes\" and \"pants\" are usually co-occur and belong to \"person\"; 2) the\ndynamic structure varies from image to image and task to task, allowing more\ncontent-/task-specific message passing among objects. To construct a VCTree, we\ndesign a score function that calculates the task-dependent validity between\neach object pair, and the tree is the binary version of the maximum spanning\ntree from the score matrix. Then, visual contexts are encoded by bidirectional\nTreeLSTM and decoded by task-specific models. We develop a hybrid learning\nprocedure which integrates end-task supervised learning and the tree structure\nreinforcement learning, where the former's evaluation result serves as a\nself-critic for the latter's structure exploration. Experimental results on two\nbenchmarks, which require reasoning over contexts: Visual Genome for scene\ngraph generation and VQA2.0 for visual Q&A, show that VCTree outperforms\nstate-of-the-art results while discovering interpretable visual context\nstructures.\n",
  "id": "arxiv.1812.01880",
  "url": "https://arxiv.org/abs/1812.01880",
  "pdf": "https://arxiv.org/pdf/1812.01880",
  "source": "arxiv.org",
  "date": 1544920817,
  "tags": []
}