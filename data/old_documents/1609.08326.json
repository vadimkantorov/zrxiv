{
  "title": "Asynchronous Stochastic Gradient Descent with Delay Compensation",
  "author": [
    "Shuxin Zheng",
    "Qi Meng",
    "Taifeng Wang",
    "Wei Chen",
    "Nenghai Yu",
    "Zhi-Ming Ma",
    "Tie-Yan Liu"
  ],
  "abstract": "  With the fast development of deep learning, it has become common to learn big\nneural networks using massive training data. Asynchronous Stochastic Gradient\nDescent (ASGD) is widely adopted to fulfill this task for its efficiency, which\nis, however, known to suffer from the problem of delayed gradients. That is,\nwhen a local worker adds its gradient to the global model, the global model may\nhave been updated by other workers and this gradient becomes \"delayed\". We\npropose a novel technology to compensate this delay, so as to make the\noptimization behavior of ASGD closer to that of sequential SGD. This is\nachieved by leveraging Taylor expansion of the gradient function and efficient\napproximation to the Hessian matrix of the loss function. We call the new\nalgorithm Delay Compensated ASGD (DC-ASGD). We evaluated the proposed algorithm\non CIFAR-10 and ImageNet datasets, and the experimental results demonstrate\nthat DC-ASGD outperforms both synchronous SGD and asynchronous SGD, and nearly\napproaches the performance of sequential SGD.\n",
  "id": "1609.08326",
  "date": 1541590451,
  "url": "https://arxiv.org/abs/1609.08326",
  "tags": [
    "giant"
  ]
}