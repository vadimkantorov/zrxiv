{
  "title": "Stochastic Deep Networks",
  "author": [
    "Gwendoline de Bie",
    "Gabriel Peyr√©",
    "Marco Cuturi"
  ],
  "abstract": "  Machine learning is increasingly targeting areas where input data cannot be\naccurately described by a single vector, but can be modeled instead using the\nmore flexible concept of random vectors, namely probability measures or more\nsimply point clouds of varying cardinality. Using deep architectures on\nmeasures poses, however, many challenging issues. Indeed, deep architectures\nare originally designed to handle fixedlength vectors, or, using recursive\nmechanisms, ordered sequences thereof. In sharp contrast, measures describe a\nvarying number of weighted observations with no particular order. We propose in\nthis work a deep framework designed to handle crucial aspects of measures,\nnamely permutation invariances, variations in weights and cardinality.\nArchitectures derived from this pipeline can (i) map measures to measures -\nusing the concept of push-forward operators; (ii) bridge the gap between\nmeasures and Euclidean spaces - through integration steps. This allows to\ndesign discriminative networks (to classify or reduce the dimensionality of\ninput measures), generative architectures (to synthesize measures) and\nrecurrent pipelines (to predict measure dynamics). We provide a theoretical\nanalysis of these building blocks, review our architectures' approximation\nabilities and robustness w.r.t. perturbation, and try them on various\ndiscriminative and generative tasks.\n",
  "id": "arxiv.1811.07429",
  "url": "https://arxiv.org/abs/1811.07429",
  "pdf": "https://arxiv.org/pdf/1811.07429",
  "source": "arxiv.org",
  "date": 1542719292,
  "tags": []
}