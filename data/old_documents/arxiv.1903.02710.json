{
  "title": "Concurrent Meta Reinforcement Learning",
  "authors": [
    "Emilio Parisotto",
    "Soham Ghosh",
    "Sai Bhargav Yalamanchi",
    "Varsha Chinnaobireddy",
    "Yuhuai Wu",
    "Ruslan Salakhutdinov"
  ],
  "abstract": "State-of-the-art meta reinforcement learning algorithms typically assume the\nsetting of a single agent interacting with its environment in a sequential\nmanner. A negative side-effect of this sequential execution paradigm is that,\nas the environment becomes more and more challenging, and thus requiring more\ninteraction episodes for the meta-learner, it needs the agent to reason over\nlonger and longer time-scales. To combat the difficulty of long time-scale\ncredit assignment, we propose an alternative parallel framework, which we name\n\"Concurrent Meta-Reinforcement Learning\" (CMRL), that transforms the temporal\ncredit assignment problem into a multi-agent reinforcement learning one. In\nthis multi-agent setting, a set of parallel agents are executed in the same\nenvironment and each of these \"rollout\" agents are given the means to\ncommunicate with each other. The goal of the communication is to coordinate, in\na collaborative manner, the most efficient exploration of the shared task the\nagents are currently assigned. This coordination therefore represents the\nmeta-learning aspect of the framework, as each agent can be assigned or assign\nitself a particular section of the current task's state space. This framework\nis in contrast to standard RL methods that assume that each parallel rollout\noccurs independently, which can potentially waste computation if many of the\nrollouts end up sampling the same part of the state space. Furthermore, the\nparallel setting enables us to define several reward sharing functions and\nauxiliary losses that are non-trivial to apply in the sequential setting. We\ndemonstrate the effectiveness of our proposed CMRL at improving over sequential\nmethods in a variety of challenging tasks.",
  "id": "arxiv.1903.02710",
  "url": "https://arxiv.org/abs/1903.02710",
  "pdf": "https://arxiv.org/pdf/1903.02710",
  "bibtex": "@misc{parisotto2019_arxiv:1903.02710,\n    title = {Concurrent Meta Reinforcement Learning},\n    author = {Emilio Parisotto, Soham Ghosh, Sai Bhargav Yalamanchi, Varsha Chinnaobireddy, Yuhuai Wu, Ruslan Salakhutdinov},\n    year = {2019},\n    archiveprefix = {arXiv},\n    eprint = {1903.02710},\n    pdf = {https://arxiv.org/pdf/1903.02710},\n    url = https://arxiv.org/abs/1903.02710\n}",
  "source": "arxiv.org",
  "date": 1552061404,
  "tags": []
}