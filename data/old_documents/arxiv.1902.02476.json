{
  "title": "A Simple Baseline for Bayesian Uncertainty in Deep Learning",
  "authors": [
    "Wesley Maddox",
    "Timur Garipov",
    "Pavel Izmailov",
    "Dmitry Vetrov",
    "Andrew Gordon Wilson"
  ],
  "abstract": "  We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose\napproach for uncertainty representation and calibration in deep learning.\nStochastic Weight Averaging (SWA), which computes the first moment of\nstochastic gradient descent (SGD) iterates with a modified learning rate\nschedule, has recently been shown to improve generalization in deep learning.\nWith SWAG, we fit a Gaussian using the SWA solution as the first moment and a\nlow rank plus diagonal covariance also derived from the SGD iterates, forming\nan approximate posterior distribution over neural network weights; we then\nsample from this Gaussian distribution to perform Bayesian model averaging. We\nempirically find that SWAG approximates the shape of the true posterior, in\naccordance with results describing the stationary distribution of SGD iterates.\nMoreover, we demonstrate that SWAG performs well on a wide variety of computer\nvision tasks, including out of sample detection, calibration, and transfer\nlearning, in comparison to many popular alternatives including MC dropout, KFAC\nLaplace, and temperature scaling.\n",
  "id": "arxiv.1902.02476",
  "url": "https://arxiv.org/abs/1902.02476",
  "pdf": "https://arxiv.org/pdf/1902.02476",
  "bibtex": "@misc{maddox2019_arxiv:1902.02476,\n    title = {A Simple Baseline for Bayesian Uncertainty in Deep Learning},\n    author = {Wesley Maddox, Timur Garipov, Pavel Izmailov, Dmitry Vetrov, Andrew Gordon Wilson},\n    year = {2019},\n    archiveprefix = {arXiv},\n    eprint = {1902.02476},\n    pdf = {https://arxiv.org/pdf/1902.02476},\n    url = {https://arxiv.org/abs/1902.02476}\n}",
  "source": "arxiv.org",
  "date": 1549923724,
  "tags": []
}