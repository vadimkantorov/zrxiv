{
  "title": "Normalized Cut Loss for Weakly-supervised CNN Segmentation",
  "author": [
    "Meng Tang",
    "Abdelaziz Djelouah",
    "Federico Perazzi",
    "Yuri Boykov",
    "Christopher Schroers"
  ],
  "abstract": "  Most recent semantic segmentation methods train deep convolutional neural\nnetworks with fully annotated masks requiring pixel-accuracy for good quality\ntraining. Common weakly-supervised approaches generate full masks from partial\ninput (e.g. scribbles or seeds) using standard interactive segmentation methods\nas preprocessing. But, errors in such masks result in poorer training since\nstandard loss functions (e.g. cross-entropy) do not distinguish seeds from\npotentially mislabeled other pixels. Inspired by the general ideas in\nsemi-supervised learning, we address these problems via a new principled loss\nfunction evaluating network output with criteria standard in \"shallow\"\nsegmentation, e.g. normalized cut. Unlike prior work, the cross entropy part of\nour loss evaluates only seeds where labels are known while normalized cut\nsoftly evaluates consistency of all pixels. We focus on normalized cut loss\nwhere dense Gaussian kernel is efficiently implemented in linear time by fast\nBilateral filtering. Our normalized cut loss approach to segmentation brings\nthe quality of weakly-supervised training significantly closer to fully\nsupervised methods.\n",
  "id": "arxiv.1804.01346",
  "url": "https://arxiv.org/abs/1804.01346",
  "pdf": "https://arxiv.org/pdf/1804.01346",
  "source": "arxiv.org",
  "date": 1542653883,
  "tags": [
    "weaksup"
  ]
}