{
  "title": "A Convergence Analysis of Gradient Descent for Deep Linear Neural\n  Networks",
  "author": [
    "Sanjeev Arora",
    "Nadav Cohen",
    "Noah Golowich",
    "Wei Hu"
  ],
  "abstract": "  We analyze speed of convergence to global optimum for gradient descent\ntraining a deep linear neural network (parameterized as $x\\mapsto W_N \\cdots\nW_1x$) by minimizing the $\\ell_2$ loss over whitened data. Convergence at a\nlinear rate is guaranteed when the following hold: (i) dimensions of hidden\nlayers are at least the minimum of the input and output dimensions; (ii) weight\nmatrices at initialization are approximately balanced; and (iii) the initial\nloss is smaller than the loss of any rank-deficient solution. The assumptions\non initialization (conditions (ii) and (iii)) are necessary, in the sense that\nviolating any one of them may lead to convergence failure. Moreover, in the\nimportant case of output dimension 1, i.e. scalar regression, they are met, and\nthus convergence to global optimum holds, with constant probability under a\nrandom initialization scheme. Our results significantly extend previous\nanalyses, e.g., of deep linear residual networks (Bartlett et al., 2018).\n",
  "id": "1810.02281",
  "date": 1541701285,
  "url": "https://arxiv.org/abs/1810.02281",
  "tags": []
}