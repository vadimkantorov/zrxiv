{
  "title": "Attentive Neural Processes",
  "authors": [
    "Hyunjik Kim",
    "Andriy Mnih",
    "Jonathan Schwarz",
    "Marta Garnelo",
    "Ali Eslami",
    "Dan Rosenbaum",
    "Oriol Vinyals",
    "Yee Whye Teh"
  ],
  "abstract": "  Neural Processes (NPs) (Garnelo et al 2018a;b) approach regression by\nlearning to map a context set of observed input-output pairs to a distribution\nover regression functions. Each function models the distribution of the output\ngiven an input, conditioned on the context. NPs have the benefit of fitting\nobserved data efficiently with linear complexity in the number of context\ninput-output pairs, and can learn a wide family of conditional distributions;\nthey learn predictive distributions conditioned on context sets of arbitrary\nsize. Nonetheless, we show that NPs suffer a fundamental drawback of\nunderfitting, giving inaccurate predictions at the inputs of the observed data\nthey condition on. We address this issue by incorporating attention into NPs,\nallowing each input location to attend to the relevant context points for the\nprediction. We show that this greatly improves the accuracy of predictions,\nresults in noticeably faster training, and expands the range of functions that\ncan be modelled.\n",
  "id": "arxiv.1901.05761",
  "url": "https://arxiv.org/abs/1901.05761",
  "pdf": "https://arxiv.org/pdf/1901.05761",
  "bibtex": "@misc{kim2019_arxiv:1901.05761,\n    title = {Attentive Neural Processes},\n    author = {Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosenbaum, Oriol Vinyals, Yee Whye Teh},\n    year = {2019},\n    archiveprefix = {arXiv},\n    eprint = {1901.05761},\n    pdf = {https://arxiv.org/pdf/1901.05761},\n    url = {https://arxiv.org/abs/1901.05761}\n}",
  "source": "arxiv.org",
  "date": 1550583039,
  "tags": []
}