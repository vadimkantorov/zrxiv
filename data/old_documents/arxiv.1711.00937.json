{
  "title": "Neural Discrete Representation Learning",
  "authors": [
    "Aaron van den Oord",
    "Oriol Vinyals",
    "Koray Kavukcuoglu"
  ],
  "abstract": "Learning useful representations without supervision remains a key challenge\nin machine learning. In this paper, we propose a simple yet powerful generative\nmodel that learns such discrete representations. Our model, the Vector\nQuantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways:\nthe encoder network outputs discrete, rather than continuous, codes; and the\nprior is learnt rather than static. In order to learn a discrete latent\nrepresentation, we incorporate ideas from vector quantisation (VQ). Using the\nVQ method allows the model to circumvent issues of \"posterior collapse\" --\nwhere the latents are ignored when they are paired with a powerful\nautoregressive decoder -- typically observed in the VAE framework. Pairing\nthese representations with an autoregressive prior, the model can generate high\nquality images, videos, and speech as well as doing high quality speaker\nconversion and unsupervised learning of phonemes, providing further evidence of\nthe utility of the learnt representations.",
  "id": "arxiv.1711.00937",
  "url": "https://arxiv.org/abs/1711.00937",
  "pdf": "https://arxiv.org/pdf/1711.00937",
  "bibtex": "@misc{oord2017_arxiv:1711.00937,\n    title = {Neural Discrete Representation Learning},\n    author = {Aaron van den Oord, Oriol Vinyals, Koray Kavukcuoglu},\n    year = {2017},\n    archiveprefix = {arXiv},\n    eprint = {1711.00937},\n    pdf = {https://arxiv.org/pdf/1711.00937},\n    url = https://arxiv.org/abs/1711.00937\n}",
  "source": "arxiv.org",
  "date": 1552480468,
  "tags": [],
  "api": "https://export.arxiv.org/api/query?id_list=1711.00937"
}