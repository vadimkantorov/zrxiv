{
  "title": "Ladder Variational Autoencoders",
  "authors": [
    "Casper Kaae Sønderby",
    "Tapani Raiko",
    "Lars Maaløe",
    "Søren Kaae Sønderby",
    "Ole Winther"
  ],
  "abstract": "Variational Autoencoders are powerful models for unsupervised learning.\nHowever deep models with several layers of dependent stochastic variables are\ndifficult to train which limits the improvements obtained using these highly\nexpressive models. We propose a new inference model, the Ladder Variational\nAutoencoder, that recursively corrects the generative distribution by a data\ndependent approximate likelihood in a process resembling the recently proposed\nLadder Network. We show that this model provides state of the art predictive\nlog-likelihood and tighter log-likelihood lower bound compared to the purely\nbottom-up inference in layered Variational Autoencoders and other generative\nmodels. We provide a detailed analysis of the learned hierarchical latent\nrepresentation and show that our new inference model is qualitatively different\nand utilizes a deeper more distributed hierarchy of latent variables. Finally,\nwe observe that batch normalization and deterministic warm-up (gradually\nturning on the KL-term) are crucial for training variational models with many\nstochastic layers.",
  "id": "arxiv.1602.02282",
  "url": "https://arxiv.org/abs/1602.02282",
  "pdf": "https://arxiv.org/pdf/1602.02282",
  "bibtex": "@misc{sønderby2016_arxiv:1602.02282,\n    title = {Ladder Variational Autoencoders},\n    author = {Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, Ole Winther},\n    year = {2016},\n    archiveprefix = {arXiv},\n    eprint = {1602.02282},\n    pdf = {https://arxiv.org/pdf/1602.02282},\n    url = https://arxiv.org/abs/1602.02282\n}",
  "source": "arxiv.org",
  "date": 1551574378,
  "tags": []
}