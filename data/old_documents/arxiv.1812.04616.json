{
  "title": "Von Mises-Fisher Loss for Training Sequence to Sequence Models with Continuous Outputs",
  "authors": [
    "Sachin Kumar",
    "Yulia Tsvetkov"
  ],
  "abstract": "  The Softmax function is used in the final layer of nearly all existing\nsequence-to-sequence models for language generation. However, it is usually the\nslowest layer to compute which limits the vocabulary size to a subset of most\nfrequent types; and it has a large memory footprint. We propose a general\ntechnique for replacing the softmax layer with a continuous embedding layer.\nOur primary innovations are a novel probabilistic loss, and a training and\ninference procedure in which we generate a probability distribution over\npre-trained word embeddings, instead of a multinomial distribution over the\nvocabulary obtained via softmax. We evaluate this new class of\nsequence-to-sequence models with continuous outputs on the task of neural\nmachine translation. We show that our models obtain upto 2.5x speed-up in\ntraining time while performing on par with the state-of-the-art models in terms\nof translation quality. These models are capable of handling very large\nvocabularies without compromising on translation quality. They also produce\nmore meaningful errors than in the softmax-based models, as these errors\ntypically lie in a subspace of the vector space of the reference translations.\n",
  "id": "arxiv.1812.04616",
  "url": "https://arxiv.org/abs/1812.04616",
  "pdf": "https://arxiv.org/pdf/1812.04616",
  "source": "arxiv.org",
  "date": 1544748517,
  "tags": []
}