{
  "title": "Large Memory Layers with Product Keys",
  "authors": [
    "Guillaume Lample",
    "Alexandre Sablayrolles",
    "Marc'Aurelio Ranzato",
    "Ludovic Denoyer",
    "Hervé Jégou"
  ],
  "abstract": "This paper introduces a structured memory which can be easily integrated into\na neural network. The memory is very large by design and therefore\nsignificantly increases the capacity of the architecture, by up to a billion\nparameters with a negligible computational overhead. Its design and access\npattern is based on product keys, which enable fast and exact nearest neighbor\nsearch. The ability to increase the number of parameters while keeping the same\ncomputational budget lets the overall system strike a better trade-off between\nprediction accuracy and computation efficiency both at training and test time.\nThis memory layer allows us to tackle very large scale language modeling tasks.\nIn our experiments we consider a dataset with up to 30 billion words, and we\nplug our memory layer in a state-of-the-art transformer-based architecture. In\nparticular, we found that a memory augmented model with only 12 layers\noutperforms a baseline transformer model with 24 layers, while being twice\nfaster at inference time. We release our code for reproducibility purposes.",
  "id": "arxiv.1907.05242",
  "url": "https://arxiv.org/abs/1907.05242",
  "pdf": "https://arxiv.org/pdf/1907.05242",
  "bibtex": "@misc{lample2019_arxiv:1907.05242,\n    title = {Large Memory Layers with Product Keys},\n    author = {Guillaume Lample and Alexandre Sablayrolles and Marc'Aurelio Ranzato and Ludovic Denoyer and Hervé Jégou},\n    year = {2019},\n    archiveprefix = {arXiv},\n    eprint = {1907.05242},\n    pdf = {https://arxiv.org/pdf/1907.05242},\n    url = {https://arxiv.org/abs/1907.05242}\n}",
  "source": "arxiv.org",
  "date": 1563010174,
  "tags": [],
  "api": "https://export.arxiv.org/api/query?id_list=1907.05242"
}