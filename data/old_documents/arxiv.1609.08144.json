{
  "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
  "authors": [
    "Yonghui Wu",
    "Mike Schuster",
    "Zhifeng Chen",
    "Quoc V. Le",
    "Mohammad Norouzi",
    "Wolfgang Macherey",
    "Maxim Krikun",
    "Yuan Cao",
    "Qin Gao",
    "Klaus Macherey",
    "Jeff Klingner",
    "Apurva Shah",
    "Melvin Johnson",
    "Xiaobing Liu",
    "Łukasz Kaiser",
    "Stephan Gouws",
    "Yoshikiyo Kato",
    "Taku Kudo",
    "Hideto Kazawa",
    "Keith Stevens",
    "George Kurian",
    "Nishant Patil",
    "Wei Wang",
    "Cliff Young",
    "Jason Smith",
    "Jason Riesa",
    "Alex Rudnick",
    "Oriol Vinyals",
    "Greg Corrado",
    "Macduff Hughes",
    "Jeffrey Dean"
  ],
  "abstract": "Neural Machine Translation (NMT) is an end-to-end learning approach for\nautomated translation, with the potential to overcome many of the weaknesses of\nconventional phrase-based translation systems. Unfortunately, NMT systems are\nknown to be computationally expensive both in training and in translation\ninference. Also, most NMT systems have difficulty with rare words. These issues\nhave hindered NMT's use in practical deployments and services, where both\naccuracy and speed are essential. In this work, we present GNMT, Google's\nNeural Machine Translation system, which attempts to address many of these\nissues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder\nlayers using attention and residual connections. To improve parallelism and\ntherefore decrease training time, our attention mechanism connects the bottom\nlayer of the decoder to the top layer of the encoder. To accelerate the final\ntranslation speed, we employ low-precision arithmetic during inference\ncomputations. To improve handling of rare words, we divide words into a limited\nset of common sub-word units (\"wordpieces\") for both input and output. This\nmethod provides a good balance between the flexibility of \"character\"-delimited\nmodels and the efficiency of \"word\"-delimited models, naturally handles\ntranslation of rare words, and ultimately improves the overall accuracy of the\nsystem. Our beam search technique employs a length-normalization procedure and\nuses a coverage penalty, which encourages generation of an output sentence that\nis most likely to cover all the words in the source sentence. On the WMT'14\nEnglish-to-French and English-to-German benchmarks, GNMT achieves competitive\nresults to state-of-the-art. Using a human side-by-side evaluation on a set of\nisolated simple sentences, it reduces translation errors by an average of 60%\ncompared to Google's phrase-based production system.",
  "id": "arxiv.1609.08144",
  "url": "https://arxiv.org/abs/1609.08144",
  "pdf": "https://arxiv.org/pdf/1609.08144",
  "bibtex": "@misc{wu2016_arxiv:1609.08144,\n    title = {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},\n    author = {Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},\n    year = {2016},\n    archiveprefix = {arXiv},\n    eprint = {1609.08144},\n    pdf = {https://arxiv.org/pdf/1609.08144},\n    url = {https://arxiv.org/abs/1609.08144}\n}",
  "source": "arxiv.org",
  "date": 1561715946,
  "tags": [],
  "api": "https://export.arxiv.org/api/query?id_list=1609.08144"
}