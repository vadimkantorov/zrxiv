{
  "title": "Large Scale Structure of Neural Network Loss Landscapes",
  "authors": [
    "Stanislav Fort",
    "Stanislaw Jastrzebski"
  ],
  "abstract": "There are many surprising and perhaps counter-intuitive properties of\noptimization of deep neural networks. We propose and experimentally verify a\nunified phenomenological model of the loss landscape that incorporates many of\nthem. High dimensionality plays a key role in our model. Our core idea is to\nmodel the loss landscape as a set of high dimensional \\emph{wedges} that\ntogether form a large-scale, inter-connected structure and towards which\noptimization is drawn. We first show that hyperparameter choices such as\nlearning rate, network width and $L_2$ regularization, affect the path\noptimizer takes through the landscape in a similar ways, influencing the large\nscale curvature of the regions the optimizer explores. Finally, we predict and\ndemonstrate new counter-intuitive properties of the loss-landscape. We show an\nexistence of low loss subspaces connecting a set (not only a pair) of\nsolutions, and verify it experimentally. Finally, we analyze recently popular\nensembling techniques for deep networks in the light of our model.",
  "id": "arxiv.1906.04724",
  "url": "https://arxiv.org/abs/1906.04724",
  "pdf": "https://arxiv.org/pdf/1906.04724",
  "bibtex": "@misc{fort2019_arxiv:1906.04724,\n    title = {Large Scale Structure of Neural Network Loss Landscapes},\n    author = {Stanislav Fort and Stanislaw Jastrzebski},\n    year = {2019},\n    archiveprefix = {arXiv},\n    eprint = {1906.04724},\n    pdf = {https://arxiv.org/pdf/1906.04724},\n    url = {https://arxiv.org/abs/1906.04724}\n}",
  "source": "arxiv.org",
  "date": 1561154991,
  "tags": [],
  "api": "https://export.arxiv.org/api/query?id_list=1906.04724"
}