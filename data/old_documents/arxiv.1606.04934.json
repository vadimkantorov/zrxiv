{
  "title": "Improving Variational Inference with Inverse Autoregressive Flow",
  "authors": [
    "Diederik P. Kingma",
    "Tim Salimans",
    "Rafal Jozefowicz",
    "Xi Chen",
    "Ilya Sutskever",
    "Max Welling"
  ],
  "abstract": "  The framework of normalizing flows provides a general strategy for flexible\nvariational inference of posteriors over latent variables. We propose a new\ntype of normalizing flow, inverse autoregressive flow (IAF), that, in contrast\nto earlier published flows, scales well to high-dimensional latent spaces. The\nproposed flow consists of a chain of invertible transformations, where each\ntransformation is based on an autoregressive neural network. In experiments, we\nshow that IAF significantly improves upon diagonal Gaussian approximate\nposteriors. In addition, we demonstrate that a novel type of variational\nautoencoder, coupled with IAF, is competitive with neural autoregressive models\nin terms of attained log-likelihood on natural images, while allowing\nsignificantly faster synthesis.\n",
  "id": "arxiv.1606.04934",
  "url": "https://arxiv.org/abs/1606.04934",
  "pdf": "https://arxiv.org/pdf/1606.04934",
  "bibtex": "@misc{kingma2016_arxiv:1606.04934,\n    title = {Improving Variational Inference with Inverse Autoregressive Flow},\n    author = {Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, Max Welling},\n    year = {2016},\n    archiveprefix = {arXiv},\n    eprint = {1606.04934},\n    pdf = {https://arxiv.org/pdf/1606.04934},\n    url = {https://arxiv.org/abs/1606.04934}\n}",
  "source": "arxiv.org",
  "date": 1550495579,
  "tags": []
}