{
  "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
  "authors": [
    "Yinhan Liu",
    "Myle Ott",
    "Naman Goyal",
    "Jingfei Du",
    "Mandar Joshi",
    "Danqi Chen",
    "Omer Levy",
    "Mike Lewis",
    "Luke Zettlemoyer",
    "Veselin Stoyanov"
  ],
  "abstract": "Language model pretraining has led to significant performance gains but\ncareful comparison between different approaches is challenging. Training is\ncomputationally expensive, often done on private datasets of different sizes,\nand, as we will show, hyperparameter choices have significant impact on the\nfinal results. We present a replication study of BERT pretraining (Devlin et\nal., 2019) that carefully measures the impact of many key hyperparameters and\ntraining data size. We find that BERT was significantly undertrained, and can\nmatch or exceed the performance of every model published after it. Our best\nmodel achieves state-of-the-art results on GLUE, RACE and SQuAD. These results\nhighlight the importance of previously overlooked design choices, and raise\nquestions about the source of recently reported improvements. We release our\nmodels and code.",
  "id": "arxiv.1907.11692",
  "url": "https://arxiv.org/abs/1907.11692",
  "pdf": "https://arxiv.org/pdf/1907.11692",
  "bibtex": "@misc{liu2019_arxiv:1907.11692,\n    title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},\n    author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},\n    year = {2019},\n    archiveprefix = {arXiv},\n    eprint = {1907.11692},\n    pdf = {https://arxiv.org/pdf/1907.11692},\n    url = {https://arxiv.org/abs/1907.11692}\n}",
  "source": "arxiv.org",
  "date": 1564466651,
  "tags": [],
  "api": "https://export.arxiv.org/api/query?id_list=1907.11692"
}