{
  "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition",
  "authors": [
    "Daniel S. Park",
    "William Chan",
    "Yu Zhang",
    "Chung-Cheng Chiu",
    "Barret Zoph",
    "Ekin D. Cubuk",
    "Quoc V. Le"
  ],
  "abstract": "We present SpecAugment, a simple data augmentation method for speech\nrecognition. SpecAugment is applied directly to the feature inputs of a neural\nnetwork (i.e., filter bank coefficients). The augmentation policy consists of\nwarping the features, masking blocks of frequency channels, and masking blocks\nof time steps. We apply SpecAugment on Listen, Attend and Spell networks for\nend-to-end speech recognition tasks. We achieve state-of-the-art performance on\nthe LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work.\nOn LibriSpeech, we achieve 6.8% WER on test-other without the use of a language\nmodel, and 5.8% WER with shallow fusion with a language model. This compares to\nthe previous state-of-the-art hybrid system of 7.5% WER. For Switchboard, we\nachieve 7.2%/14.6% on the Switchboard/CallHome portion of the Hub5'00 test set\nwithout the use of a language model, and 6.8%/14.1% with shallow fusion, which\ncompares to the previous state-of-the-art hybrid system at 8.3%/17.3% WER.",
  "id": "arxiv.1904.08779",
  "url": "https://arxiv.org/abs/1904.08779",
  "pdf": "https://arxiv.org/pdf/1904.08779",
  "bibtex": "@misc{park2019_arxiv:1904.08779,\n    title = {SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition},\n    author = {Daniel S. Park and William Chan and Yu Zhang and Chung-Cheng Chiu and Barret Zoph and Ekin D. Cubuk and Quoc V. Le},\n    year = {2019},\n    archiveprefix = {arXiv},\n    eprint = {1904.08779},\n    pdf = {https://arxiv.org/pdf/1904.08779},\n    url = {https://arxiv.org/abs/1904.08779}\n}",
  "source": "arxiv.org",
  "date": 1558092938,
  "tags": [],
  "api": "https://export.arxiv.org/api/query?id_list=1904.08779"
}